{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the Indexer class\n",
        "class Indexer:\n",
        "    def __init__(self, labels):\n",
        "        self.objs_to_ints = {label: i for i, label in enumerate(labels)}\n",
        "        self.ints_to_objs = labels\n",
        "\n",
        "    def get_index(self, obj, add=True):\n",
        "        if obj not in self.objs_to_ints:\n",
        "            if add:\n",
        "                self.objs_to_ints[obj] = len(self.ints_to_objs)\n",
        "                self.ints_to_objs.append(obj)\n",
        "            else:\n",
        "                return None\n",
        "        return self.objs_to_ints[obj]\n",
        "\n",
        "    def get_object(self, idx):\n",
        "        return self.ints_to_objs[idx]\n",
        "\n",
        "\n",
        "# Define feature extraction functions\n",
        "def extract_features(sentence):\n",
        "    features_by_pair = []\n",
        "\n",
        "    for i in range(len(sentence) - 1):\n",
        "        features = []\n",
        "\n",
        "        # Feature function for specific tag transitions (e.g., B-PER to I-PER)\n",
        "        if sentence[i + 1][1][0] == 'I' and sentence[i][1][0] == 'B' and sentence[i][1][2:] == sentence[i + 1][1][2:]:\n",
        "            features.append(1)\n",
        "        else:\n",
        "            features.append(0)\n",
        "\n",
        "        # Feature function for current tag being the same as the previous tag\n",
        "        features.append(int(sentence[i][1] == sentence[i + 1][1]))\n",
        "\n",
        "        # Feature function for transitions between entity types\n",
        "        entity_types = ['PER', 'LOC', 'ORG', 'MISC', 'O']\n",
        "        tag_pairs = product(entity_types, repeat=2)\n",
        "        for prev_entity, curr_entity in tag_pairs:\n",
        "            if len(sentence[i][1]) > 1 and len(sentence[i + 1][1]) > 1 and sentence[i][1][2:] == prev_entity and sentence[i + 1][1][2:] == curr_entity:\n",
        "                features.append(1)\n",
        "            elif len(sentence[i][1]) > 1 and len(sentence[i + 1][1]) == 1 and sentence[i][1][2:] == prev_entity and sentence[i + 1][1] == curr_entity:\n",
        "                features.append(1)\n",
        "            elif len(sentence[i][1]) == 1 and len(sentence[i + 1][1]) > 1 and sentence[i][1] == prev_entity and sentence[i + 1][1][2:] == curr_entity:\n",
        "                features.append(1)\n",
        "            elif len(sentence[i][1]) == 1 and len(sentence[i + 1][1]) == 1 and sentence[i][1] == prev_entity and sentence[i + 1][1] == curr_entity:\n",
        "                features.append(1)\n",
        "            else:\n",
        "                features.append(0)\n",
        "\n",
        "        features_by_pair.append(features)\n",
        "\n",
        "    return features_by_pair\n",
        "\n",
        "# Function to read CoNLL 2003 data\n",
        "def read_conll_2003(file_path):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                if len(current_sentence) > 1:  # Check if sentence length is greater than 1\n",
        "                    sentences.append(current_sentence)\n",
        "                current_sentence = []\n",
        "            else:\n",
        "                parts = line.split(' ')\n",
        "                word = parts[0]\n",
        "                tag = parts[-1]\n",
        "                current_sentence.append((word, tag))\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Define the LinearChainCRF class\n",
        "class LinearChainCRF:\n",
        "    def __init__(self, num_labels, feature_size):\n",
        "        self.num_labels = num_labels\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        # Define model parameters (weights)\n",
        "        self.weights = tf.Variable(tf.random.normal([1, feature_size]))\n",
        "\n",
        "    def compute_score(self, features):\n",
        "        # Calculate the unnormalized scores using TensorFlow\n",
        "        scores = tf.matmul(self.weights, tf.transpose(features))\n",
        "        return scores\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Forward algorithm to calculate the log partition function\n",
        "        scores = self.compute_score(features)\n",
        "        scores = tf.transpose(scores)\n",
        "\n",
        "        alphas = [scores[0]]\n",
        "        for t in range(1, tf.shape(scores)[0]):\n",
        "            alpha_t = alphas[t - 1] + tf.reduce_logsumexp(scores, axis=1)\n",
        "            alphas.append(alpha_t + scores[t])\n",
        "\n",
        "        log_partition = tf.reduce_logsumexp(alphas[-1])\n",
        "\n",
        "        return log_partition\n",
        "\n",
        "    def viterbi_decode(self, features):\n",
        "        # Viterbi decoding to find the best sequence\n",
        "        scores = self.compute_score(features)\n",
        "        scores = tf.transpose(scores)\n",
        "\n",
        "        viterbi_scores = [scores[0]]\n",
        "        backpointers = []\n",
        "\n",
        "        for t in range(1, tf.shape(scores)[0]):\n",
        "            viterbi_t = viterbi_scores[t - 1] + scores\n",
        "            backpointer_t = tf.argmax(viterbi_t, axis=1)\n",
        "            viterbi_t = tf.reduce_max(viterbi_t, axis=1)\n",
        "\n",
        "            viterbi_scores.append(viterbi_t)\n",
        "            backpointers.append(backpointer_t)\n",
        "\n",
        "        best_last_label = tf.argmax(viterbi_scores[-1])\n",
        "        best_sequence = [best_last_label]\n",
        "\n",
        "        for backpointers_t in reversed(backpointers):\n",
        "            best_last_label = tf.gather(backpointers_t, best_last_label)\n",
        "            best_sequence.insert(0, best_last_label)\n",
        "\n",
        "        return best_sequence\n",
        "\n",
        "\n",
        "    def negative_log_likelihood(self, features, labels):\n",
        "        # Calculate the negative log-likelihood using TensorFlow\n",
        "        scores = self.compute_score(features)\n",
        "\n",
        "        # Flatten the scores tensor\n",
        "        flat_scores = tf.reshape(scores, [-1])\n",
        "\n",
        "\n",
        "        # labeled_scores = tf.gather(flat_scores, indices)\n",
        "\n",
        "        # Calculate the log partition function using the forward algorithm\n",
        "        log_partition = self.forward(features)\n",
        "\n",
        "        log_likelihood = tf.reduce_sum(flat_scores) - log_partition\n",
        "        return -log_likelihood\n",
        "\n",
        "    # ... (other methods)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train_step(self, features, labels, optimizer):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.negative_log_likelihood(features, labels)\n",
        "\n",
        "        gradients = tape.gradient(loss, [self.weights])\n",
        "        optimizer.apply_gradients(zip(gradients, [self.weights]))\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def save_weights(self, checkpoint_path):\n",
        "        # Save the model weights using tf.train.Checkpoint\n",
        "        checkpoint = tf.train.Checkpoint(model=self)\n",
        "        checkpoint.save(checkpoint_path)\n",
        "\n",
        "    def load_weights(self, checkpoint_path):\n",
        "        # Restore the model weights using tf.train.Checkpoint\n",
        "        checkpoint = tf.train.Checkpoint(model=self)\n",
        "        checkpoint.restore(checkpoint_path)\n",
        "\n",
        "# Main function to prepare data, extract features, and train the model\n",
        "def prepare_data_and_train_crf(train_file_path, num_epochs=10):\n",
        "    # Read CoNLL 2003 data\n",
        "    train_sentences = read_conll_2003(train_file_path)\n",
        "\n",
        "    # Extract features for each sentence\n",
        "    features_list = [extract_features(sentence) for sentence in train_sentences]\n",
        "\n",
        "    # Convert features to TensorFlow constants\n",
        "    features_tf = [tf.constant(features, dtype=tf.float32) for features in features_list]\n",
        "\n",
        "    labels_1 = ['I-LOC', 'B-ORG', 'O', 'B-PER', 'I-PER', 'I-MISC', 'B-MISC', 'I-ORG', 'B-LOC']\n",
        "    indexer = Indexer(labels_1)\n",
        "\n",
        "    # Initialize CRF model\n",
        "    num_labels = 9  # Replace with the actual number of labels\n",
        "    feature_size = 27\n",
        "    crf = LinearChainCRF(num_labels, feature_size)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for features, sentence in zip(features_tf, train_sentences):\n",
        "\n",
        "            labels = [indexer.get_index(tag) for _, tag in sentence]\n",
        "            labels = tf.constant(labels, dtype=tf.int32)\n",
        "            loss = crf.train_step(features, labels, optimizer)\n",
        "            total_loss += loss\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss.numpy()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "train_file_path = '/content/train.txt'\n",
        "prepare_data_and_train_crf(train_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I93D5gCN0h8H",
        "outputId": "1b5a9513-4818-4d5a-b69b-33c5963eccca"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: -311363.1875\n",
            "Epoch 2, Loss: -1187892.125\n",
            "Epoch 3, Loss: -2062892.0\n",
            "Epoch 4, Loss: -2936997.25\n",
            "Epoch 5, Loss: -3811049.0\n",
            "Epoch 6, Loss: -4685089.5\n",
            "Epoch 7, Loss: -5559133.0\n",
            "Epoch 8, Loss: -6433126.0\n",
            "Epoch 9, Loss: -7307153.0\n",
            "Epoch 10, Loss: -8181192.0\n"
          ]
        }
      ]
    }
  ]
}